{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de comentarios de películas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importacion de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Librerias para preprocesamiento de texto\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords as stopwords\n",
    "from nltk.tokenize import word_tokenize as word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_web_sm')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del csv\n",
    "file = './data/MovieReviews.csv'\n",
    "raw = pd.read_csv(file, sep=',')\n",
    "reviews = raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_es</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Si está buscando una película de guerra típica...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Supongo que algunos directores de películas de...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Es difícil contarle más sobre esta película si...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          review_es sentimiento\n",
       "0           0  Si está buscando una película de guerra típica...    positivo\n",
       "1           1  Supongo que algunos directores de películas de...    positivo\n",
       "2           2  Es difícil contarle más sobre esta película si...    positivo"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la columna de Unnnamed\n",
    "reviews.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_es      0\n",
       "sentimiento    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nulos en el dataset\n",
    "reviews.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola   c mo est s \n"
     ]
    }
   ],
   "source": [
    "# Funcion para eliminar caracteres especiales y numeros\n",
    "def clean_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "print(clean_text('Hola, ¿cómo estás?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para eliminar caracteres especiales y numeros\n",
    "def clean_text(text):\n",
    "    # Pasara a minusculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Regex para eliminar caracteres especiales y numeros\n",
    "    special_chars = re.compile('[^a-zA-Z\\s]')\n",
    "\n",
    "    # Eliminar caracteres especiales y numeros\n",
    "    tokens = [special_chars.sub('', word) for word in tokens]\n",
    "\n",
    "    # Eliminar stopwords\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "    # Eliminar stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "    # Reunir los tokens\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "print(clean_text('Hola, ¿cómo estás?'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para preparar tus datos para utilizar el algoritmo de Naive Bayes en el análisis de sentimientos, necesitas realizar algunas tareas de preprocesamiento de los datos. Los pasos típicos de preprocesamiento incluyen:\n",
    "\n",
    "- Limpiar el texto: esto implica eliminar cualquier información no relevante para el análisis de sentimientos, como signos de puntuación, números, caracteres especiales y palabras vacías (como \"a\", \"el\", \"y\", etc.). Puedes hacer uso de librerías de procesamiento de texto como NLTK o SpaCy para llevar a cabo esta tarea.\n",
    "- Tokenizar el texto: esto implica dividir el texto en unidades más pequeñas, como palabras individuales. Puedes hacer uso de las mismas librerías que en el paso anterior.\n",
    "- Crear una matriz de términos: una vez que tienes tus textos limpios y tokenizados, necesitas crear una matriz que represente la frecuencia de cada término en cada texto. Puedes hacer uso de la clase CountVectorizer de Scikit-Learn para crear esta matriz.\n",
    "- Dividir los datos en conjuntos de entrenamiento y prueba: debes dividir tus datos en conjuntos de entrenamiento y prueba para poder evaluar el rendimiento del modelo. Puedes hacer uso de la función train_test_split de Scikit-Learn para llevar a cabo esta tarea.\n",
    "- Aplicar una transformación Tfidf: Puedes aplicar una transformación Tf-idf a la matriz de términos que creaste en el paso 3, esto ayuda a reducir el peso de palabras muy comunes que no aportan información valiosa al modelo. Scikit-Learn tiene una clase TfidfTransformer que puedes utilizar para aplicar esta transformación.\n",
    "- Una vez que hayas completado estos pasos, estarás listo para entrenar y evaluar tu modelo de Naive Bayes. Puedes seguir los pasos que mencioné anteriormente para utilizar la implementación de Scikit-Learn del algoritmo de Naive Bayes para clasificación de texto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro, para realizar el primer paso de la preparación de los datos, que es la limpieza del texto, puedes seguir los siguientes pasos:\n",
    "\n",
    "- Eliminar signos de puntuación: utiliza expresiones regulares para eliminar signos de puntuación como comas, puntos, comillas, etc. Estos signos de puntuación no aportan información valiosa para el análisis de sentimientos.\n",
    "- Eliminar caracteres especiales: elimina cualquier otro carácter especial que no sea un signo de puntuación, como @, #, $, etc.\n",
    "- Convertir todo el texto a minúsculas: esto ayuda a evitar problemas de inconsistencia en el texto, ya que \"Hola\" y \"hola\" se considerarán diferentes palabras si no se convierten a minúsculas.\n",
    "- Eliminar números: los números no aportan información relevante para el análisis de sentimientos, por lo que es una buena práctica eliminarlos.\n",
    "- Eliminar palabras vacías: las palabras vacías, como \"a\", \"el\", \"y\", \"o\", etc., son muy comunes en el lenguaje natural pero no aportan información valiosa para el análisis de sentimientos. Puedes hacer uso de librerías de procesamiento de texto como NLTK o SpaCy para eliminar estas palabras.\n",
    "- Aplicar stemming: stemming es el proceso de reducir las palabras a su forma base o raíz. Esto ayuda a reducir la cantidad de palabras diferentes que se usan en un texto y simplifica el análisis. NLTK tiene una clase SnowballStemmer que puedes utilizar para llevar a cabo esta tarea.\n",
    "- Una vez que hayas completado estos pasos, tendrás tus textos limpios y listos para el siguiente paso en la preparación de los datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar palabras vacías, puedes hacer uso de las librerías NLTK o SpaCy en Python. Aquí te presento un ejemplo de cómo hacerlo con NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "text = \"Este es un ejemplo de texto con palabras vacías que deben ser eliminadas\"\n",
    "words = text.split()\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tokenizar el texto utilizando las librerías NLTK o SpaCy en Python, puedes seguir los siguientes ejemplos:\n",
    "\n",
    "Para tokenizar con NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Este es un ejemplo de texto que vamos a tokenizar.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementación algoritmos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Algoritmo Nathalia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Algoritmo Sergio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Algoritmo Calixto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análisis de resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
